---
title: "Продвинутые способы визуализации данных и эксплораторный анализ в R"
author: "Дмитрий Серебренников"
output: 
  html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Загрузим данные и пакеты

```{r}
# Загрузим библиотеки
library(dplyr)
library(ggplot2)
theme_set(theme_minimal())
library(ggpubr)

pima <- read.csv('pima.csv') 

# Сделаем более детализированную переменную возрастных групп
pima <- pima %>% 
  mutate(
    diabetes_ch = as.character(diabetes),
    age_group = case_when(
      age < 31 ~ "21-30",
      age >= 31 & age < 41 ~ "31-40",
      age >= 41 & age < 51 ~ "41-50",
      age >= 51 & age < 61 ~ "51-60",
      age >= 61 ~ "60+"
    ))
table(pima$age_group)
```

...






# Plotly - Интерактивные графики

`plotly` - это специальная библиотека для создания интерактивных графиков
со своим синтаксисом (очень похожим на ggplot). С ней можно
познакомиться [здесь](https://plotly.com/r/).


## Основные правила синтаксиса

Загрузим библиотеку: 

```{r}
library(plotly)
```

Разберём базовые примеры. Для более подробного изучения приглашаю [сюда](https://plotly.com/r/)

### Box plot:

```{r}
plot_ly(
  pima[pima$insulin != 0,],
  x = ~insulin,
  color = ~age_group,
  type = "box"
)
```

### Scatter plot

```{r}
plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose)
```

## Начнём усложнять

Добавим название и настройки маркеров: 

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
  x = ~ insulin,
  y = ~ glucose,
  marker = list(
    size = 10,
    # размер
    color = 'rgba(255, 182, 193, .9)',
    # Цвет внутри
    line = list(color = 'rgba(152, 0, 0, .8)',  # Цвет окружности
                width = 2)
  )
)   %>%
  layout(
    title = 'Отношение уровня глюкозы и инсулина в данных PIMA',
    yaxis = list(title = 'Уровень глюкозы',
                 zeroline = FALSE),  # Уберём выделения нулевых осей по y
    xaxis = list(title = 'Уровень инсулина',
                 zeroline = FALSE)) # Уберём выделения нулевых осей по y

```


Раскрасим по диабет-статусу:


```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
  x = ~ insulin,
  y = ~ glucose,
  color = ~diabetes_ch
)   %>%
  layout(
    title = 'Отношение уровня глюкозы и инсулина в данных PIMA',
    yaxis = list(title = 'Уровень глюкозы',
                 zeroline = FALSE),  # Уберём выделения нулевых осей по y
    xaxis = list(title = 'Уровень инсулина',
                 zeroline = FALSE)) # Уберём выделения нулевых осей по y

```


Добавим третью ось:

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0) & (pima$mass != 0),],
  x = ~ insulin,
  y = ~ glucose,
  z = ~mass,
  color = ~diabetes_ch
)   %>%
  layout(
    title = 'Отношение уровня глюкозы и инсулина в данных PIMA',
    yaxis = list(title = 'Уровень глюкозы',
                 zeroline = FALSE),  # Уберём выделения нулевых осей по y
    xaxis = list(title = 'Уровень инсулина',
                 zeroline = FALSE)) # Уберём выделения нулевых осей по y

```



### Palette / Цветовые гаммы

```{r}
# Со стандартной палеткой
plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose,
        color = ~age_group)

# С изменённой палеткой
plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose,
        color = ~age_group,
        colors = "Set1")
```

Палетки можно сделать самостоятельно (аналогичным образом это делается в ggplot): 

```{r}
pal <- c("red", "blue", "green")
pal <- setNames(pal, levels(pima$age_group)) # age_group - должна быть factor. Количество цветов в pal должно соответствовать числу уровней факторов

plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose,
        color = ~age_group,
        colors = pal)
```



### Цвет, размер точек и подписи к ним

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),], 
  x = ~insulin, 
  y = ~glucose,
  color = ~diabetes_ch, 
  size = ~mass,
  text = ~mass, 
  hoverinfo = "text")
```


### Анимированный график

Построим анимацию для разных возрастов. Она не будет особо осмысленна (т.к. делается обычно для данных по времени), но мы учимся:

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
  x = ~insulin, 
  y = ~glucose, 
  color = ~diabetes_ch, 
  size = ~mass,
  text = ~mass, 
  frame = ~age, # По какой колонке нам сделать анимацию/интерактивность?
  hoverinfo = "text",
  type = 'scatter',
  mode = 'markers'
) 
```


### Почему мы не учим plotly в первую очередь?

Он не гибкий. Если ggplot понимает что вам нужно на ходу, то для plotly вам нужно подготовить данные в том виде в котором вы хотите их визуализировать.



*ЗАДАНИЕ*

В чанке ниже напишите код 3d графика по осям age, mass, pressure:

```{r}

```

...






## ggplotly

Когда вы хотите представить итоговый результат в интерактивном формате (например, с помощью странички на сайте сделанной в `Rmd`), то её лучше писать в `plotly.` Однако, если вы работаете в формате эксплораторного анализа, то можно воспользоваться "обёрткой" `plotly` для вашего `ggplot` графика. Сделать это очень просто.

Сделаем и сохраним обычный график:

```{r}
plot <- pima %>% 
  filter(mass != 0 & triceps != 0) %>% 
  ggplot(aes(x=mass, y=triceps, color = diabetes)) + 
  geom_point(size=3) +
  theme_minimal()
```

Подгрузим plotly:

```{r}
library(plotly)
```

"Обернём" наш график:

```{r}
ggplotly(plot)
```

*ЗАДАНИЕ*

В чанке ниже напишите код любого графика и сделайте его интерактивным:

```{r}

```

...







# Визуализация тестов

Посмотрим базовые описательные статистики:

```{r}
library(rstatix)
library(ggpubr)

pima %>% 
  filter(pressure > 5) %>% 
  get_summary_stats(pressure, type = "mean_sd")
```

Поймем, можно ли делать параметрические оценки. Сделаем QQ-plot.

```{r}
ggqqplot(pima[pima$pressure>5,], 
         x = "pressure", facet.by = "age_group")
```

Можем. Сделаем t-test в фреймворке ggplot:

```{r}
stat.test <- pima[pima$pressure>5,] %>% 
  t_test(pressure ~ diabetes_ch) %>% 
  add_xy_position(x = "diabetes_ch")
stat.test
```

Визуализируем результаты:

```{r}
ggboxplot(
  pima[pima$pressure>5,], 
  x = "diabetes_ch", y = "pressure", 
  ylab = "Pressure", xlab = "Diabete", 
  add = "jitter"
  ) + 
  labs(subtitle = get_test_label(stat.test, detailed = TRUE)) + 
  stat_pvalue_manual(stat.test, tip.length = 0) 
```







# Анализ корреляций

Когда мы говорим об отношениях переменных друг к другу, мы так или иначе
приходим к языку корреляции. Т.е. статистической взаимосвязи двух или
более случайных величин при которых изменения в одной переменной связаны
с изменением во второй.

На всякий случай привожу формулу рассчёта коэффициента корреляции
Пирсона:
$r_{xy}=\frac{\Sigma(x_i-\bar{x})\times(y_i-\bar{y})}{\sqrt{\Sigma(x_i-\bar{x})^2\times\Sigma(y_i-\bar{y})^2}}$

Он может принимать значения от -1 (отрицательная связь), до 1
(положительная связь).


В R можно строить матрицу корреляций для всех численных переменных
нашего датасета с помощью пакета `corrplot`. Загрузим его:

```{r}
library(corrplot)
```

Сама матрица строится в 2 этапа.

1.  Получаем объект матрицы:

```{r}
# Для более "чистого" результата, избавляемся от ошибочных значений
pima_clear <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 ) %>% 
  select(is.integer | is.numeric) # Обратите внимание, в dplyr можно задавать выборку колонок через команды определения формата данных

# Сделаем ещё один "чистый" датафрейм, но со значениями диабет статуса. Он понадобится нам в самом конце занятия 
pima_clear_with_ch <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 )


head(pima_clear)
```

```{r}
# Получаем непосредственно матрицу
pima_cor <- cor(pima_clear) 
pima_cor
```

2.  Визуализируем её в corplot:

```{r}
corrplot(pima_cor, method = 'number')
```

У представления такой матрицы может быть ещё большое количество
вариаций. Их можно посомтреть
[здесь](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

Например: 

```{r}
corrplot(pima_cor, method = "color", type = "lower", 
         addCoef.col = "grey30", diag = FALSE,
         cl.pos = "b", tl.col = "grey10",
         col = COL2('RdBu', 10))
```


Кроме того, я рекомендую посмотреть также на пакет
[corrr](https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr).

В нём реализованы более дателизированные функции для анализа корреляций.
Например функция ниже показывает не только корреляционные взаимосвязи,
но и отношения близости переменных друг к другу с точки зрения сетевого
анализа:

```{r}
library(corrr)

pima_cor %>% 
  network_plot(min_cor = .0)
```

Такие графики осмысленно использовать тогда когда у вас небольшое (до
20) количество переменных.

...


## Матричные графики

С помощью таких графиков можно получить информацию как о распределении отдельных количественных переменных, так и о взаимосвязи внутри каждой пары количественных переменных.

Можно использовать функцию `ggpairs` из пакета `GGally`: 

```{r}
library(GGally)

ggpairs(pima_clear,
        title = 'Correlations in PIMA dataset',progress = F) +
    theme_minimal() +
    scale_fill_manual(values = c('#69b3a2')) +
    scale_colour_manual(values = c('#69b3a2'))
```

...






# Heat map

## Анализ двух категориальных переменных

В базовом варианте, heat map используют для иллюстрации отношения двух
категориальных переменных друг с другом. Для этого, нужно сначала
агрегировать их значения:

```{r}
pima_aggr <- pima %>% 
  group_by(age_group, diabetes) %>% 
  summarise(N = n())
```

Визуализируем:

```{r}
pima_aggr %>% 
  ggplot(aes(x = age_group, y = diabetes, fill = N)) +
  geom_tile(color = "black") +
  geom_text(aes(label = N), color = "white", size = 4) +
  coord_fixed()
```


## Анализ наблюдений

Второй способ использовать heat map - представлять сами наблюдения по
какому-то универсальному признаку.

Например, мы можем применить процедуру стандартизации значений для всех
наблюдений по численным переменным и визуализировать его.

Стандартизируем значения:

```{r}
pima_clear_scaled <- scale(pima_clear)
head(pima_clear_scaled)
```

Визуализируем с помощью расширения к ggplot - ggfortify(). В чем
проблема такого графика?

```{r}
library(ggfortify) 

autoplot(pima_clear_scaled)
```

...





# Поиск схожих наблюдений. Лирическое отступление об основах кластерного анализа

## Иерархическая кластеризация (hierarchical clustering)

Относится к семейству аггломеративных кластеризаций
(agglomerative/agnes, есть ещё другой вид - divisive/diana), но является
самой простой из них - не требует выделить число кластеров заранее.
Нужна в первую очередь, чтобы увидеть сруктуру отнесения наблюдений к
кластерам (и подумать о том, как вообще устроенны наши наблюдения).

Принцип работы - каждое наблюдение находит ближайшее себе по дистанции
до него (по каждой из переменных). После нахождения первого соседа,
начинатся следующий шаг, когда образовавшийся мини-кластер ищет другие
ближайшие к ним кластеры/наблюдения. Так прохожит несколько шагов, пока
все наблюдения не сольются в один кластер.

"Длинна" дистанции обозначает меру схожести наблюдений - Высоту/Height
(Площадь Восстания ближе к ЕУ чем Кудрово).

Посмотрим визуально на
[картинке](https://drive.google.com/file/d/1vzzHg4GwWd6bQKMx1Y_7yOs39NTEKHQo/view?usp=sharing).

...

Есть много способов измерять расстояние. Посмотрим примеры разнообразных
метрик:

![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*FTVRr_Wqz-3_k6Mk6G4kew.png)

Мы будем пользоваться базовым - Евклидовым расстоянием.

**Как с этим работать?**

1.  Установик и загрузим библиотеку для визуализации кластеров

```{r}
library(factoextra)
```

2.  Создаём матрицу дистанций

```{r}
pima_clear_dist <- dist(pima_clear_scaled, 
                        method = "euclidean"
                        )
as.matrix(pima_clear_dist)[1:6,1:6]
```

3.  Высчитываем дендрограмму кластеров. Обратите внимание на аргумент
    method. Методов для кластеризации достаточно много. Мы используем
    один из базовых - "ward.D2"

```{r}
pima_clear_hc <- hclust(d = pima_clear_dist, 
                        method = "ward.D2")
```

4.  Визуализируем (на больших данных может занять какое-то время)

```{r}
fviz_dend(pima_clear_hc, 
          cex = 0.1) # cex() - размер лейблов
```

Далее можно углубляться в способы визуализации и оценки таких
результатов. Рекомендую посмотреть на учебник по
[ссылке](https://drive.google.com/file/d/1SlbmfQbHWNoHxSLI-Hn_UUfZTxE9x3_A/view?usp=sharing).

...

# Heat map + Tree map?

Мы затронули основы кластеризации, чтобы понять, как работать в тех
случаях, когда нам нужно упорядочить данные, сгруппировав наблюдения.
Совместив это с heat map мы можем выделить группы наблюдений по
каким-либо признакам.

Сделать это просто через пакет pheatmap.

Загрузим его.

```{r}
library(pheatmap)
```

Визуализируем heat map и tree map одновременно:

```{r}
pheatmap(pima_clear_scaled, 
         show_rownames = FALSE, 
         clustering_distance_rows = pima_clear_dist,
         clustering_method = "ward.D2", 
         cutree_rows = 5,
         cutree_cols = length(colnames(pima_clear_scaled)),
         angle_col = 45, 
         main = "Dendrograms for clustering rows and columns with heatmap")
```

...







# Principal component analysis (PCA)

Сейчас мы уже можем выделить определённые группы пациентов. Однако даже
на таком маленьком объёме данных (напомню, мы работаем с 392
наблюдениями) график почти не читаем. Что делать?

Одно из самых популярных решений - метод главных компонент (principal
component analysis или PCA).

...

## Теория и учебный пример

Чтобы разбраться с методом рассмотрим какие две проблемы он решает и как
он это делает:

1.  Если в данных есть ряд скоррелированных переменных (например, в
    нашем случае - mass и triceps), то они не дают провести корректные
    оценки данных и выделение паттернов.

    Почему?

2.  Если в данных очень много наблюдений и переменных, то при
    разведывательном анализе необходимо сократить их размер, не потеряв
    в качестве и полноте интерпретаций. Эту задачу выполняют методы
    уменьшения размерности и PCA относится к ним.

...

Рассмотрим учебный пример. Допустим у нас в данных есть только колонки
mass и triceps. Мы хотим с одной стороны снять корреляцию между ними, а
с другой - уменьшить размерность, т.е. представить их одной переменной.

    Как нам это сделать?

...

**Мы можем сконструировать новые переменные из двух имеющихся и сняв
корреляцию и уменьшив размерность!**

```{r}
# Подготовим данные
pima_example <- pima_clear %>% 
  select(mass, triceps)

# Визуализируем
ggplot() +
  geom_point(data = pima_example, aes(x = mass, y = triceps)) +
  theme_minimal()

# Что значит такое распределение точек? 
```

**Сконструируем переменные, в которых не будет корреляции и которые
(делаем из бага фичу!) помогут уменьшить размерность**

Если мы хотим привести две переменные к одной, то мы должны провести
какое-то действие с этими переменными. Например, сложить!

```{r}
pima_example <- pima_example %>% 
  mutate(pc1 = mass + triceps) # Создаем первую главную компоненту
```

А давайте теперь поэксперементируем с вычитанием

```{r}
pima_example <- pima_example %>% 
  mutate(pc2 = mass - triceps)
```

Теперь ещё раз вспомним как выглядит график:

```{r}
ggplot() +
  geom_point(data = pima_example, aes(x = mass, y = triceps)) + 
  theme_minimal() 
```

    А теперь, следим за руками...

    А что если мы просто сделаем колонки, которые только что создали (pc1, pc2) нашими новыми координатными осями?
    Т.е. вместо x будет pc1, вместо y - pc2, а точки перерисуются исходя из нашего нового представления. 

**Вспоминаем две проблемы, с которых мы начинали. Как такое
преобразование помогает решить обе сложности?**

...

Чтобы не быть голословными, перерисуем график, но уже с новыми
координатами - pc1 и pc2:

```{r}
ggplot() +
  geom_point(data = pima_example, aes(x = pc1, y = pc2)) +
  theme_minimal() 

# Догадайтесь, где теперь лежат средние линии наших старых координат (mass, triceps)?
# Кажется корреляция осталась, а что сделать с переменными, что занулить её?
```

Вопросы на понимание:

-   А зачем мы это сделали? Какие следствия такого подхода?

-   Если мы хотим выразить две наши переменные какой-то одной, то что мы
    выберем pc1 или pc2?

...

### Подойдём к вопросу более формально

-   Нам нужно найти оси, которые приведут корреляцию к нулю и при этом,
    создадут макисмальную полноту разброса переменных. Т.е. фактически,
    в нашей задаче мы решем два уравнения:

$$
PC1 = (alpha_1 * mass) + (beta_1 * triceps);

PC2 = (alpha_2 * mass) + (beta_2 * triceps)
$$

...где alpha и beta - это коэффициенты, которые позволяют добиться
минимальной скоррелированности mass и triceps. (читай - найти такой угол
для новых осей координат, где в данных будет максимальный разброс при
минимальной корреляции)

Воспользуемся специальной функцией для этого:

```{r}
pima_example <- pima_example %>% 
  select(mass, triceps) # Оставим только нужные нам две переменные

pima.pca <- prcomp(pima_example, 
                scale = T) # Нужно ли нормировать? TRUE|FALSE. Фактически, мы могли бы подать на функцию pima_example_scaled и тогда следовало поставить scale = F, веть нормирование в том датафрейме уже сделано!
```


```{}
NB: Помните, что нормирование перед работой с PCA делает этот метод затруднительным для работы с бинарными переменными (почему?). У этого есть ряд выходов. Для подсчёта матрицы дистанций можно использовать gower distance. А вместо PCA использовать FAMD (Factorial Analysis of Mixed Data), основанный на комбинации PCA для количественных переменных и MCA (Multiple Correspondence Analysis) для категориальных.
```



Посмотрим на веса наших главных компонент:

```{r}
pima.pca$rotation
```

Rotation показывает коэффициенты разворота новых осей относительно
старых. Таким образом, решением нашего уравнения будет:

$$
PC1 = (0.707 * mass) + (-0.707 * triceps); 

PC2 = (0.707 * mass) + (0.707 * triceps)
$$


*(странность весов обусловлена тем, что мы работаем с учебным примером,
где всего лишь 2 переменные)*

Нарисуем компоненты на графике:

```{r}
ggplot() +
  geom_point(data = pima.pca$x, # Новые переменные лежат здесь
             aes(x = PC1, y = PC2)) +
  theme_minimal() 
```

Но как функция высчитала эти цифры? 

1. Она находит центр распределения данных по среднему двух переменных; 

2. От него строит множество прямых линий с каждым разом делая угол наклона всё больше; 

3. Для каждой линии считается сумма квадратов расстояний от всех точек в данных до линии; 

4. Выбирается линия, где эта сумма будет минимальной. Она и становится новой главной компонентой; 

5. Если алгоритм уже отобрал однй главную компоненту, он начинает отбирать следующую так, чтобы каждая следующая линия имела наименьшую корреляцию со всеми предыдущими; 

6. Алгоритм останавливается, тогда, когда количесто найденных главных компонент не
сравняется с количеством переменных.

...

    Но в чём суть, если мы хотели уменьшить количество переменных, а оно (количество главных компонент) осталось таким же?

    - Да, осталось, но только вот главные компоненты концептуально неравны друг другу. Каждая следующая из них объясняет меньше предыдущей. Считается, что алгоритм сработал хорошо, когда 70% вариации данных укладывается в 3 компоненты. 

    ...Т.е. компонент может быть десятки, но первые 3 агреггируют в себе большую часть сложности наших данных. 

Это легко заметить уже даже в случае трёх переменных (и трёх компонент).
Рассмотрим прекрасный материал Гарика Мороза и соавторов по ссылке
[здесь](http://math-info.hse.ru/f/2015-16/ling-mag-quant/lecture-pca.html),
кроме того я совутую [эту интерактивную визуализацию
PCA](https://bryanhanson.github.io/LearnPCA/articles/Vig_05_Visualizing_PCA_3D.html)
или [эту](https://setosa.io/ev/principal-component-analysis/).

    ```{r}
    # Кстати, мы также можем делать 3d визуализацию с помощью plotly:
    plot_ly(data = pima_clear, 
            x=~mass, 
            y=~pressure, 
            z=~triceps, 
            size = 1,
            type="scatter3d", mode="markers")
    # Но это совсем другая тема...
    ```

...

## Практика и реальный пример

Проведём PCA уже на полных данных pima

```{r}
# Загрузим библиотеки
library(FactoMineR)
```

Делаем PCA:

```{r}
pima_full.pca <- prcomp(pima_clear, 
                        scale = T) # Не забываем про стандартизацию!
```

Оценим результат.

```{r}
summary(pima_full.pca)
```

Смотрим на "Cumulative Proportion". У нас в данных первые 4 главные
компоненты объясняют 74% вариации данных. Посмотрим это на графике:

```{r}
fviz_eig(pima_full.pca, addlabels = T, ylim = c(0, 40))
```

На самом деле, это не слишком хороший результат, т.к. следующая
конвенционально важная отметка в 90% достигается уже только на PC7.
Бывают данные, которые не слишком хороши для PCA, но нельзя сказать, что
у нас всё ужасно. Первые две компоненты объясняют 50% дисперсии. Вокруг
них и будет сосредоточен основной анализ.

### Анализ переменных по PCA

Мы можем начать анализировать, как наши переменные связаны с PC1 и PC2.
Посмотрим на график ниже:

```{r}
fviz_pca_var(pima_full.pca, col.var = "contrib")
```

Подсказка для интерпретации графика
[здесь](https://bioinfo4all.files.wordpress.com/2021/01/principal-component-analysis-pca-1.png?w=2048).

-   Стрелки - средние значения переменных для PC1 (Dim1) и PC2 (Dim2). В
    скобках указаны проценты объяснённой дисперсии каждой из двух
    компонент. На каждую последующую PC всегда приходится всё меньше и
    меньше разброса в данных.
-   Цвет и близость к кругу - насколько та или иная переменная вносит
    вклад в анализируемые главные компоненты
-   Направление - относительная мера близости переменных. Если стрелки
    расходятся в прямо-противоположные стороны, то переменные
    отрицательно скоррелированы внутри представленных главных компонент.

В данных мы видим три группы переменных: 

* age, pregnant. 
* mass, triceps, pedigree 
* остальные

    Теперь оценим PC1 и PC2 в целом. Что "схватили" первая и вторая компоненты? какой разброс мы видим по осям x и y?

...

Мы также можем отдельно посмотреть на, например, топ 3 самых важных
переменных с т.зр. их вариации в PC1 и PC2:

```{r}
fviz_pca_var(pima_full.pca, 
             select.var = list(contrib = 3), # Задаём число здесь 
             col.var = "contrib")
```

    У самих по себе главных компонент есть одна очень большая проблема с точки зрения их анализа. Какая?

Посмотрим из чего состоят 1, 2 и 3 главные компоненты:

```{r}
fviz_contrib(pima_full.pca, choice = "var", axes = 1, top = 24) # 1
fviz_contrib(pima_full.pca, choice = "var", axes = 2, top = 24) # 2
fviz_contrib(pima_full.pca, choice = "var", axes = 3, top = 24) # 3
```

### Анализ наблюдений по PCA

Помимо переменных мы можем анализировать также и наблюдения, искать в
них кластеры и корреляцию с переменными в целом. Для этого используется
biplot. Bi потому, что на нем одновременно изображены и точки, и
переменные

```{r}
# Загрузим библиотеку
library(ggbiplot) # devtools::install_github("vqv/ggbiplot")
```

Сделаем biplot:

```{r}
ggbiplot(pima_full.pca, 
         scale=0, alpha = 0.1) + 
  theme_minimal()
```

Более осмысленным biplot становится при использовании кластерных
методов, с помощью которых мы можем разделить наблюдения на группы.
Посмотрим, наблюдается ли разница между группами по diabetes:

```{r}
# Сделаем корректные данные для группировки по diabetes.
pima_clear_with_ch <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 )

# Визуализируем с группировкой по diabetes (для этого переменную нужно сделать фактором)
ggbiplot(pima_full.pca, 
         scale=0, 
         groups = as.factor(pima_clear_with_ch$diabetes), 
         ellipse = T,
         alpha = 0.2) +
  theme_minimal()
```

А что с возрастными группами:

```{r}
ggbiplot(pima_full.pca, 
         scale=0, 
         groups = as.factor(pima_clear_with_ch$age_group), 
         ellipse = T,
         alpha = 0.2) +
  theme_minimal()
```

Для дальнейшего ознакомления с PCA я рекомендую посмотреть следующие
туториалы: [1](https://bioinfo4all.wordpress.com/2021/01/31/tutorial-6-how-to-do-principal-component-analysis-pca-in-r/), [2](https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff), [3](https://juliasilge.com/blog/best-hip-hop/), [4](https://juliasilge.com/blog/cocktail-recipes-umap/)


...



# UMAP

PCA - отличный метод, когда мы хотим одновременно понять соотношения
колонок и строк, но часто бывают ситуации, когда вам лучше понять
близость строк друг к другу и сделать "сгустки" наблюдений, а не
разряженные облака. Для таких задач применяют UMAP.

UMAP (Uniform Manifold Approximation and Projection) - это алгоритм
уменьшения размерности, основанный на методах теории топологии. В
отличие от PCA, он оценивает не глобальное отношение переменных, а
локальную близость строк. Сначала мы оцениваем многомерное пространство,
а затем по-очереди начинаем уменьшать его размерность, но так, чтобы при
каждом следующем снижении изначально близкие друг другу точки
становились ещё ближе (образуем своеобразные воронки).

Важно (!), UMAP имеет тенденцию сохранять локальные расстояния между
точками в ущерб глобальному отображению (т.е. имеет тенденцию создавать
"сгустки", а не разреженные облака). Благодаря этому он отлично подходит
для понимания структуры отношения строк, но при этом он даёт крайне
ограниченное количество способов анализировать колонки.

Вы можете прочитать детальнее
[здесь](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).

### Tidymodels approach

```{r, message=FALSE,warning=FALSE}
library(tidymodels)
library(embed)

umap_prep <- recipe(~., data = pima_clear) %>% # "техническая" строка, нужная для работы фреймворка tidymodels
  step_normalize(all_predictors()) %>% # нормируем все колонки
  step_umap(all_predictors()) %>%  # проводим в UMAP. Используем стандартные настройки. Чтобы менять ключевой параметр (neighbors), нужно больше погружаться в машинное обучение
  prep() %>%  # "техническая" строка, нужная для работы фреймворка tidymodels. Мы выполняем все степы выше 
  juice() # Финальная строка - приводим результаты UMAP к стандартизированному датасету

```

Визуализиуем два первых измерения UMAP и добавим информацию о возрастных
группах и диабет-статусе:

```{r}
umap_prep %>%
  ggplot(aes(UMAP1, UMAP2)) + #  # можно добавить раскраску 
  geom_point(aes(color = as.character(pima_clear_with_ch$age_group),
                 shape = pima_clear_with_ch$diabetes_ch), 
             alpha = 0.7, size = 2) +
  labs(color = NULL) 
```

...



# Завершение. 

**А если графики используются не для эксплораторного анализа, а для презентации результатов?**

В визуализации данных есть своя теория и свои исследования воприятия
разных графиков (например что-то вы можете почитать на [data-to-viz в
блоге](https://www.data-to-viz.com/caveats.html) или у [Анастасии
Кузнецовой](https://nastengraph.medium.com/)).

Когда график для вас - способ представить результат вашим коллегам или
широкой публике, его стоит делать исходя из несколько других
соображений, чем когда вы делаете это при эксплораторном анализе. По
этой причине, при подготовке графика следует учесть следующие принципы:

1.  Оцените насколько хорошо ваши данные подходят типу графика.

2.  Делайте фокус на чем-то одном: общих паттернах или деталях. Исходя
    из этого стоит выбирать тип графика.

3.  Агрегируйте большие объемы данных при визуализации.

4.  Правильно выбирайте палетки. Учитывайте то, как их могут читать
    дальтоники или, например, акцентируют ли цвета внимание читателя на
    том, что вам нужно?

5.  Не делайте график впечатлительным без необходимости. Эффектность в
    простоте.

6.  Убедитесь, что график соответствует интуитивным конвенциям
    восприятия (у него не перевёрнуты оси, они не обрезаны (но иногда
    это позволительно), у данных указан источник, все подписи
    унифицированны и проч.)

7.  При интерпретации результатов помните - график показывает только то,
    что он показывает. Ни один график не показывает вам причинных
    эффектов. Только связи. Correlation != Causation.

8.  Помните, что цель любого хорошего графика -- рассказать историю (и
    убедить вас в ней).

...

Я попытался рассказать историю на наших данных на графике ниже.
Попробуйте и вы!

```{r}
plot <- pima %>% 
  mutate(
    age_group = factor(age_group, levels = c("21-30", "31-40", "41-50", "51-60", "60+" )),
    diabetes = case_when(
      diabetes == 'pos' ~ "Diabet-Positive",
      diabetes == 'neg' ~ "Diabet-Negative"
    ) # Переводим в фактор, для корректной последовательности категорий
    ) %>% 
  filter(insulin != 0 & glucose != 0) %>% 
  ggplot(aes(x=insulin, y=glucose, color = age_group)) + 
  geom_point(size = 3, alpha = 0.8) + 
  facet_grid(. ~ diabetes) +
  scale_color_brewer(palette = 'OrRd') +
  ggtitle('Indian Women Diabets') +
  labs(y = 'Plasma glucose concentration (log10)', x = '2-Hours serum insulin (mu U\\ml) (log10)') + 
  guides(color = guide_legend(title = 'Age Groups')) +
  scale_x_log10() + scale_y_log10() +
  theme_minimal() 

plot
```

# Полезные ссылки по теме визуализации данных

-   [R Graph Galery](https://r-graph-gallery.com/index.html)

-   Ссылки на Телеграмм каналы:

    -   <https://t.me/revealthedata>
    -   <https://t.me/nastengraph>
    -   <https://t.me/data_publication>
    -   <https://t.me/leftjoin>
